{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GpY-ArzB1rH-"
   },
   "source": [
    "# **Disease Detection using Symptoms and Treatment recommendation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jcDTmIyqctTq"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from statistics import mean\n",
    "from nltk.corpus import wordnet \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from itertools import combinations\n",
    "from time import time\n",
    "from collections import Counter\n",
    "import operator\n",
    "from xgboost import XGBClassifier\n",
    "import math\n",
    "from Treatment import diseaseDetail\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IsuWDbumeSco"
   },
   "source": [
    "**synonyms function** finds the synonymous terms of a symptom entered by the user.\n",
    "\n",
    "This is necessary as the user may use a term for a symptom which may be different from the one present in dataset.\n",
    "This improves the accuracy by reducing the wrong predictions even when symptoms for a disease are entered slightly different than the ones on which model is trained.\n",
    "\n",
    "*Synonyms are searched on Thesaurus.com and NLTK Wordnet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhrzSJPadBwH"
   },
   "outputs": [],
   "source": [
    "# returns the list of synonyms of the input word from thesaurus.com (https://www.thesaurus.com/) and wordnet (https://www.nltk.org/howto/wordnet.html)\n",
    "def synonyms(term):\n",
    "    synonyms = []\n",
    "    response = requests.get('https://www.thesaurus.com/browse/{}'.format(term))\n",
    "    soup = BeautifulSoup(response.content,  \"html.parser\")\n",
    "    try:\n",
    "        container=soup.find('section', {'class': 'MainContentContainer'}) \n",
    "        row=container.find('div',{'class':'css-191l5o0-ClassicContentCard'})\n",
    "        row = row.find_all('li')\n",
    "        for x in row:\n",
    "            synonyms.append(x.get_text())\n",
    "    except:\n",
    "        None\n",
    "    for syn in wordnet.synsets(term):\n",
    "        synonyms+=syn.lemma_names()\n",
    "    return set(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JwigX34ldGPl"
   },
   "outputs": [],
   "source": [
    "# utlities for pre-processing\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "splitter = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t1sbUx8C22zG"
   },
   "source": [
    "**Disease Symptom dataset** was created in a separate python program.\n",
    "\n",
    "**Dataset scrapping** was done using **NHP website** and **wikipedia data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rZTXyRhNgN_O"
   },
   "source": [
    "Disease Combination dataset contains the combinations for each of the disease present in dataset as practically it is often observed that it is not necessary for a person to have a disease when all the symptoms are faced by the patient or the user.\n",
    "\n",
    "*To tackle this problem, combinations are made with the symptoms for each disease.*\n",
    "\n",
    " **This increases the size of the data exponentially and helps the model to predict the disease with much better accuracy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h1LSI08aiDTn"
   },
   "source": [
    "*df_comb -> Dataframe consisting of dataset generated by combining symptoms for each disease.* -  rows\n",
    "\n",
    "*df_norm -> Dataframe consisting of dataset which contains a single row for each diseases with all the symptoms for that corresponding disease.* - 261 rows\n",
    "\n",
    "**Dataset contains 261 diseases and their symptoms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5fzGWm5NdIkN"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'F:\\\\SEM-6\\\\Minor-2\\\\Disease-Detection-based-on-Symptoms-master\\\\Dataset\\\\dis_sym_dataset_comb.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4692\\291691141.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load Dataset scraped from NHP (https://www.nhp.gov.in/disease-a-z) & Wikipedia\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Scrapping and creation of dataset csv is done in a separate program\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_comb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"F:\\SEM-6\\Minor-2\\Disease-Detection-based-on-Symptoms-master\\Dataset\\dis_sym_dataset_comb.csv\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Disease combination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"F:\\SEM-6\\Minor-2\\Disease-Detection-based-on-Symptoms-master\\Dataset\\dis_sym_dataset_norm.csv\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Individual Disease\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'F:\\\\SEM-6\\\\Minor-2\\\\Disease-Detection-based-on-Symptoms-master\\\\Dataset\\\\dis_sym_dataset_comb.csv'"
     ]
    }
   ],
   "source": [
    "# Load Dataset scraped from NHP (https://www.nhp.gov.in/disease-a-z) & Wikipedia\n",
    "# Scrapping and creation of dataset csv is done in a separate program\n",
    "df_comb = pd.read_csv(\"F:\\SEM 6\\Minor-2\\Disease-Detection-based-on-Symptoms-master\\Dataset\\dis_sym_dataset_comb.csv\") # Disease combination\n",
    "df_norm = pd.read_csv(\"F:\\SEM 6\\Minor-2\\Disease-Detection-based-on-Symptoms-master\\Dataset\\dis_sym_dataset_norm.csv\") # Individual Disease\n",
    "\n",
    "X = df_comb.iloc[:, 1:]\n",
    "Y = df_comb.iloc[:, 0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A4fuvOxOhR6v"
   },
   "source": [
    "Using **Logistic Regression (LR) Classifier** as it gives better accuracy compared to other classification models as observed in the comparison of model accuracies in Model_latest.py\n",
    "\n",
    "Cross validation is done on dataset with cv = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Njmbkf6IdKwt"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr = lr.fit(X, Y)\n",
    "scores = cross_val_score(lr, X, Y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(lr, open('model_saved', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gvck32ifdVZV"
   },
   "outputs": [],
   "source": [
    "X = df_norm.iloc[:, 1:]\n",
    "Y = df_norm.iloc[:, 0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ppiBHtudX1O"
   },
   "outputs": [],
   "source": [
    "# List of symptoms\n",
    "dataset_symptoms = list(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total symptoms are 489"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "489"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_symptoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNytuZen1Ij3"
   },
   "source": [
    "# Symptoms initially taken from user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "6eRjwZhQdbxN",
    "outputId": "0b698f30-810b-42f2-e47b-128564d3d6fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter symptoms separated by comma(,):\n",
      "fever, sore throat, headache\n"
     ]
    }
   ],
   "source": [
    "# Taking symptoms from user as input \n",
    "user_symptoms = str(input(\"Please enter symptoms separated by comma(,):\\n\")).lower().split(',')\n",
    "# Preprocessing the input symptoms\n",
    "processed_user_symptoms=[]\n",
    "for sym in user_symptoms:\n",
    "    sym=sym.strip()\n",
    "    sym=sym.replace('-',' ')\n",
    "    sym=sym.replace(\"'\",'')\n",
    "    sym = ' '.join([lemmatizer.lemmatize(word) for word in splitter.tokenize(sym)])\n",
    "    processed_user_symptoms.append(sym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xCAJTUngi_V_"
   },
   "source": [
    "Pre-processing on symptoms entered by user is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "Pewx4v_jdcbV",
    "outputId": "0e70bb1e-bd37-4f28-924e-65a2e32f20df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After query expansion done by using the symptoms entered\n",
      "['pyrexia febricity fever febrility feverishness', 'raw throat sensitive mad sore throat painful huffy afflictive sore pharynx tender', 'worry headache vexation cephalalgia concern head ache']\n"
     ]
    }
   ],
   "source": [
    "# Taking each user symptom and finding all its synonyms and appending it to the pre-processed symptom string\n",
    "user_symptoms = []\n",
    "for user_sym in processed_user_symptoms:\n",
    "    user_sym = user_sym.split()\n",
    "    str_sym = set()\n",
    "    for comb in range(1, len(user_sym)+1):\n",
    "        for subset in combinations(user_sym, comb):\n",
    "            subset=' '.join(subset)\n",
    "            subset = synonyms(subset) \n",
    "            str_sym.update(subset)\n",
    "    str_sym.add(' '.join(user_sym))\n",
    "    user_symptoms.append(' '.join(str_sym).replace('_',' '))\n",
    "# query expansion performed by joining synonyms found for each symptoms initially entered\n",
    "print(\"After query expansion done by using the symptoms entered\")\n",
    "print(user_symptoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7sPyVlJIjdv2"
   },
   "source": [
    "The below procedure is performed in order to show the symptom synonmys found for the symptoms entered by the user.\n",
    "\n",
    "The symptom synonyms and user symptoms are matched with the symptoms present in dataset. Only the symptoms which matches the symptoms present in dataset are shown back to the user. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet is checking the similarity between a list of symptoms in a dataset and a list of user-input symptoms. It does this by looping over all the symptoms in the dataset and comparing them to the user-input symptoms. If the similarity score is greater than 0.5, the symptom from the dataset is added to a final list of found symptoms. The similarity score is calculated by counting the number of words in common between the dataset symptom and the user-input symptom and dividing it by the total number of words in the dataset symptom. Finally, the found symptoms are converted to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qVnrRYXpdelN"
   },
   "outputs": [],
   "source": [
    "# Loop over all the symptoms in dataset and check its similarity score to the synonym string of the user-input \n",
    "# symptoms. If similarity>0.5, add the symptom to the final list ... \n",
    "\n",
    "# Using jaccard similarity... \n",
    "\n",
    "found_symptoms = set()\n",
    "for idx, data_sym in enumerate(dataset_symptoms):\n",
    "    # data_sym = abdominal cramp \n",
    "    data_sym_split = data_sym.split()\n",
    "    # data_sym_split = ['abdominal', 'cramp']\n",
    "    for user_sym in user_symptoms:\n",
    "        count=0\n",
    "        for symp in data_sym_split:\n",
    "            if symp in user_sym.split():\n",
    "                count+=1\n",
    "        if count/len(data_sym_split)>0.5:\n",
    "            found_symptoms.add(data_sym)\n",
    "found_symptoms = list(found_symptoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5g0jKPRfj-dP"
   },
   "source": [
    "## **Prompt the user to select the relevant symptoms by entering the corresponding indices.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "piRNP4WSdiCG",
    "outputId": "54c1c6fb-6c7b-4b01-c69e-fd92eed19e5d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top matching symptoms from your search!\n",
      "0 : headache\n",
      "1 : painful\n",
      "2 : sore throat\n",
      "3 : fever\n",
      "\n",
      "Please select the relevant symptoms. Enter indices (separated-space):\n",
      "0 2 3\n"
     ]
    }
   ],
   "source": [
    "# Print all found symptoms\n",
    "print(\"Top matching symptoms from your search!\")\n",
    "for idx, symp in enumerate(found_symptoms):\n",
    "    print(idx,\":\",symp)\n",
    "    \n",
    "# Show the related symptoms found in the dataset and ask user to select among them\n",
    "select_list = input(\"\\nPlease select the relevant symptoms. Enter indices (separated-space):\\n\").split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find other relevant symptoms from the dataset based on user symptoms based on the highest co-occurance with the ones that is input by the user\n",
    "dis_list = set()\n",
    "final_symp = [] \n",
    "counter_list = []\n",
    "for idx in select_list:\n",
    "    # select_list = [0, 1, 2]\n",
    "    symp=found_symptoms[int(idx)]\n",
    "    # found_symptoms = ['fever', 'sore throat', 'headache', 'painful']\n",
    "    final_symp.append(symp)\n",
    "    # final_symp = ['fever', 'headache', 'sore throat']\n",
    "    # symp = fever\n",
    "    # kaunsa kaunsa disease mei fever hai ek symptom usko disease list mei daal do ...\n",
    "    dis_list.update(set(df_norm[df_norm[symp]==1]['label_dis']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet appears to be written in Python. It seems to be iterating over a list called dis_list and for each dis in the list, it is selecting a row from a DataFrame df_norm where the value in the column label_dis is equal to dis. The selected row is then converted to a list and the first element is removed. The code then iterates over the remaining elements of the row and checks if the value is not equal to 0 and if the corresponding symptom (from a list called dataset_symptoms) is not already in a list called final_symp. If both conditions are met, the symptom is appended to a list called counter_list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line of code is selecting a row from a DataFrame df_norm where the value in the column label_dis is equal to the value of the variable dis. The .loc method is used to select rows based on a boolean condition. The .values attribute is then used to extract the values of the selected row as a NumPy array, and the .tolist() method is called to convert the array to a Python list. The resulting list is then assigned to the variable row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dis in dis_list:\n",
    "    # dis = Chikungunya Fever\n",
    "    row = df_norm.loc[df_norm['label_dis'] == dis].values.tolist()\n",
    "    row[0].pop(0)\n",
    "    for idx,val in enumerate(row[0]):\n",
    "        if val!=0 and dataset_symptoms[idx] not in final_symp:\n",
    "            counter_list.append(dataset_symptoms[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the symptoms of the diseases that are present are put in a list..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BO2mW3K4oIz9"
   },
   "source": [
    "## To find symptoms which generally co-occur, for example with symptoms like cough, headache generally happens hence they co-occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet is creating a dictionary called dict_symp using the Counter class from the collections module. The Counter class takes an iterable (in this case, the list counter_list) and returns a dictionary where the keys are the unique elements in the iterable and the values are the number of times each element appears in the iterable. The code then creates a list of tuples called dict_symp_tup by calling the .items() method on dict_symp to get a view of its items (key-value pairs) and passing it to the sorted() function. The sorted() function sorts the items in ascending order based on their second element (the value) using the itemgetter() function from the operator module as the key function. The reverse=True argument is passed to the sorted() function to sort the items in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pjq2q16OdlHe"
   },
   "outputs": [],
   "source": [
    "# Symptoms that co-occur with the ones selected by user              \n",
    "dict_symp = dict(Counter(counter_list))\n",
    "dict_symp_tup = sorted(dict_symp.items(), key=operator.itemgetter(1),reverse=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TjmBiLuGpEfH"
   },
   "source": [
    "## User is presented with a list of co-occuring symptoms to select from and is performed iteratively to recommend more possible symptoms based on the similarity to the previously entered symptoms.\n",
    "\n",
    "As the co-occuring symptoms can be in overwhelming numbers, only the top 5 are recommended to the user from which user can select the symptoms.\n",
    "\n",
    "If user does not have any of those 5 symptoms and wants to see the next 5, he can do so by giving input as -1.\n",
    "\n",
    "To stop the recommendation, user needs to give input as \"No\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "colab_type": "code",
    "id": "neTz-oNVdn0N",
    "outputId": "de4c07ac-1ef6-45ae-ab10-454cd8ea425c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Common co-occuring symptoms:\n",
      "0 : testicular pain\n",
      "1 : vomiting\n",
      "2 : barky cough\n",
      "3 : confusion\n",
      "4 : maculopapular rash\n",
      "Do you have have of these symptoms? If Yes, enter the indices (space-separated), 'no' to stop, '-1' to skip:\n",
      "2\n",
      "\n",
      "Common co-occuring symptoms:\n",
      "0 : diarrhea\n",
      "1 : feeling tired\n",
      "2 : nausea\n",
      "3 : swollen lymph node\n",
      "4 : chest pain\n",
      "Do you have have of these symptoms? If Yes, enter the indices (space-separated), 'no' to stop, '-1' to skip:\n",
      "1 4\n",
      "\n",
      "Common co-occuring symptoms:\n",
      "0 : shortness breath\n",
      "1 : runny nose\n",
      "2 : muscle weakness\n",
      "3 : unintended weight loss\n",
      "4 : large lymph node\n",
      "Do you have have of these symptoms? If Yes, enter the indices (space-separated), 'no' to stop, '-1' to skip:\n",
      "0 1 2\n",
      "\n",
      "Common co-occuring symptoms:\n",
      "0 : fatigue\n",
      "1 : seizure\n",
      "2 : tiredness\n",
      "3 : red eye\n",
      "4 : dizziness\n",
      "Do you have have of these symptoms? If Yes, enter the indices (space-separated), 'no' to stop, '-1' to skip:\n",
      "0 2\n",
      "\n",
      "Common co-occuring symptoms:\n",
      "0 : non itchy skin ulcer\n",
      "1 : vaginal bleeding\n",
      "2 : skin peeling\n",
      "3 : muscle joint pain\n",
      "4 : joint bone pain\n",
      "Do you have have of these symptoms? If Yes, enter the indices (space-separated), 'no' to stop, '-1' to skip:\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "# Iteratively, suggest top co-occuring symptoms to the user and ask to select the ones applicable \n",
    "found_symptoms=[]\n",
    "count=0\n",
    "for tup in dict_symp_tup:\n",
    "    count+=1\n",
    "    found_symptoms.append(tup[0])\n",
    "    if count%5==0 or count==len(dict_symp_tup):\n",
    "        print(\"\\nCommon co-occuring symptoms:\")\n",
    "        for idx,ele in enumerate(found_symptoms):\n",
    "            print(idx,\":\",ele)\n",
    "        select_list = input(\"Do you have have of these symptoms? If Yes, enter the indices (space-separated), 'no' to stop, '-1' to skip:\\n\").lower().split();\n",
    "        if select_list[0]=='no':\n",
    "            break\n",
    "        if select_list[0]=='-1':\n",
    "            found_symptoms = [] \n",
    "            continue\n",
    "        for idx in select_list:\n",
    "            final_symp.append(found_symptoms[int(idx)])\n",
    "        found_symptoms = [] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nI5taHc8pfY3"
   },
   "source": [
    "Final Symptom list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "4jIiVsbBdpg-",
    "outputId": "e89075cf-b4ab-484f-e10b-9a4864f54c90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final list of Symptoms that will be used for prediction:\n",
      "headache\n",
      "sore throat\n",
      "fever\n",
      "barky cough\n",
      "feeling tired\n",
      "chest pain\n",
      "shortness breath\n",
      "runny nose\n",
      "muscle weakness\n",
      "fatigue\n",
      "tiredness\n"
     ]
    }
   ],
   "source": [
    "# Create query vector based on symptoms selected by the user\n",
    "print(\"\\nFinal list of Symptoms that will be used for prediction:\")\n",
    "sample_x = [0 for x in range(0,len(dataset_symptoms))]\n",
    "for val in final_symp:\n",
    "    print(val)\n",
    "    sample_x[dataset_symptoms.index(val)]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o9MbvRV_phpv"
   },
   "source": [
    "Prediction of disease is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8yWRutt2drbt"
   },
   "outputs": [],
   "source": [
    "# Predict disease\n",
    "lr = LogisticRegression()\n",
    "lr = lr.fit(X, Y)\n",
    "\n",
    "prediction = lr.predict_proba([sample_x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VpwualbypkNl"
   },
   "source": [
    "Show top k diseases and their probabilities to the user.\n",
    "\n",
    "K in this case is 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding prob of each disease and sorting it in desceing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bVcksjqSdt61"
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "diseases = list(set(Y['label_dis']))\n",
    "diseases.sort()\n",
    "topk = prediction[0].argsort()[-k:][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8_A-6Dl5qHlv"
   },
   "source": [
    "# **Showing the list of top k diseases to the user with their prediction probabilities.**\n",
    "\n",
    "# **For getting information about the suggested treatments, user can enter the corresponding index to know more details.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "bgoHqtPSdvYO",
    "outputId": "908f5f4b-0894-480e-d9d5-e72bfe36b435"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 diseases predicted based on symptoms\n",
      "0 Disease name: Influenza \tProbability: 44.55%\n",
      "1 Disease name: Common cold \tProbability: 37.13%\n",
      "2 Disease name: Coronavirus disease 2019 (COVID-19) \tProbability: 37.13%\n",
      "3 Disease name: Legionellosis \tProbability: 37.13%\n",
      "4 Disease name: Asbestos-related diseases \tProbability: 29.7%\n",
      "\n",
      "More details about the disease? Enter index of disease or '-1' to discontinue and close the system:\n",
      "0\n",
      "\n",
      "Influenza\n",
      "Other names -  Flu, the flu, Grippe \n",
      "Specialty -  Infectious disease \n",
      "Symptoms -  Fever, runny nose, sore throat, muscle pain, headache, coughing, fatigue \n",
      "Usual onset -  1–4 days after exposure \n",
      "Duration -  2–8 days \n",
      "Causes -  Influenza viruses \n",
      "Prevention -  Hand washing, flu vaccines \n",
      "Medication -  Antiviral drugs such as oseltamivir \n",
      "Frequency -  3–5 million severe cases per year   \n",
      "Deaths -  >,290,000–650,000 deaths per year   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTop {k} diseases predicted based on symptoms\")\n",
    "topk_dict = {}\n",
    "# Show top 5 highly probable disease to the user.\n",
    "for idx,t in  enumerate(topk):\n",
    "    match_sym=set()\n",
    "    row = df_norm.loc[df_norm['label_dis'] == diseases[t]].values.tolist()\n",
    "    row[0].pop(0)\n",
    "\n",
    "    for idx,val in enumerate(row[0]):\n",
    "        if val!=0:\n",
    "            match_sym.add(dataset_symptoms[idx])\n",
    "    prob = (len(match_sym.intersection(set(final_symp)))+1)/(len(set(final_symp))+1)\n",
    "    prob *= mean(scores)\n",
    "    topk_dict[t] = prob\n",
    "j = 0\n",
    "topk_index_mapping = {}\n",
    "topk_sorted = dict(sorted(topk_dict.items(), key=lambda kv: kv[1], reverse=True))\n",
    "for key in topk_sorted:\n",
    "  prob = topk_sorted[key]*100\n",
    "  print(str(j) + \" Disease name:\",diseases[key], \"\\tProbability:\",str(round(prob, 2))+\"%\")\n",
    "  topk_index_mapping[j] = key\n",
    "  j += 1\n",
    "\n",
    "select = input(\"\\nMore details about the disease? Enter index of disease or '-1' to discontinue and close the system:\\n\")\n",
    "if select!='-1':\n",
    "    dis=diseases[topk_index_mapping[int(select)]]\n",
    "    print()\n",
    "    print(diseaseDetail(dis))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SymptomSuggestion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
