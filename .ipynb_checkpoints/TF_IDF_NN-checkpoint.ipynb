{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GpY-ArzB1rH-"
   },
   "source": [
    "# **Disease Detection using Symptoms and Treatment recommendation**\n",
    "\n",
    "This notebook contains the application of Neural Net and GAN on the disease dataset generated through scrapping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "etxuEgYCG7bC",
    "outputId": "b1aa83f5-0ac1-4c9b-b532-575fc01660c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rashi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rashi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing nltk to download resources for stopwords and wordnet\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement googlesearch (from versions: none)\n",
      "ERROR: No matching distribution found for googlesearch\n"
     ]
    }
   ],
   "source": [
    "!pip install googlesearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CrLG2ksh5w4Z"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3\\lib\\site-packages\\requests\\__init__.py:78: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({0}) or chardet ({1}) doesn't match a supported \"\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'googlesearch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16888\\3287801804.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mTreatment\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdiseaseDetail\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m# ignore warnings generated due to usage of old version of tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\SEM 6\\Minor-2\\Disease-Detection-based-on-Symptoms-master\\Treatment.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgooglesearch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'googlesearch'"
     ]
    }
   ],
   "source": [
    "# importing all libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "import math\n",
    "import operator\n",
    "import pickle\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from statistics import mean\n",
    "from nltk.corpus import wordnet \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from itertools import combinations\n",
    "from time import time\n",
    "from collections import Counter\n",
    "import operator\n",
    "import warnings\n",
    "from Treatment import diseaseDetail\n",
    "# ignore warnings generated due to usage of old version of tensorflow\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t1sbUx8C22zG"
   },
   "source": [
    "**Disease Symptom dataset** was created in a separate python program.\n",
    "\n",
    "**Dataset scrapping** was done using **NHP website** and **wikipedia data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "txTu6XeVgGAK"
   },
   "outputs": [],
   "source": [
    "# Load Dataset scraped from NHP (https://www.nhp.gov.in/disease-a-z) & Wikipedia\n",
    "# Scrapping and creation of dataset csv is done in a separate program\n",
    "df=pd.read_csv(\"/content/drive/My Drive/Python Project data/IR_Project/dis_sym_dataset_norm.csv\")\n",
    "documentname_list=list(df['label_dis'])\n",
    "df=df.iloc[:,1:]\n",
    "columns_name=list(df.columns)\n",
    "documentname_list=list(documentname_list)\n",
    "\n",
    "N=len(df)\n",
    "M=len(columns_name)\n",
    "\n",
    "# All symptoms IDF\n",
    "idf={}\n",
    "for col in columns_name:\n",
    "  temp=np.count_nonzero(df[col])\n",
    "  idf[col]=np.log(N/temp)\n",
    "\n",
    "# All disease,symptom TF\n",
    "tf={}\n",
    "for i in range(N):\n",
    "  for col in columns_name:\n",
    "    key=(documentname_list[i],col)\n",
    "    tf[key]=df.loc[i,col]\n",
    "\n",
    "# All disease,symptom TF.IDF\n",
    "tf_idf={}\n",
    "for i in range(N):\n",
    "  for col in columns_name:\n",
    "    key=(documentname_list[i],col)\n",
    "    tf_idf[key]=float(idf[col])*float(tf[key])\n",
    "\n",
    "# vector of TF.IDF\n",
    "D = np.zeros((N, M),dtype='float32')\n",
    "for i in tf_idf:\n",
    "    sym = columns_name.index(i[1])\n",
    "    dis=documentname_list.index(i[0])\n",
    "    D[dis][sym] = tf_idf[i]\n",
    "\n",
    "# function for cosine dot product\n",
    "def cosine_dot(a, b):\n",
    "    if np.linalg.norm(a) == 0 or np.linalg.norm(b) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        temp = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "        return temp\n",
    "\n",
    "# convert data to lower case\n",
    "def convert_tolowercase(data):\n",
    "    return data.lower()\n",
    "\n",
    "# tokenizing using regextokenizer\n",
    "def regextokenizer_func(data):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    data = tokenizer.tokenize(data)\n",
    "    return data\n",
    "\n",
    "# function to generate query vector for tf_idf\n",
    "def gen_vector(tokens):\n",
    "    Q = np.zeros(M)\n",
    "    counter = Counter(tokens)\n",
    "    query_weights = {}\n",
    "    for token in np.unique(tokens):\n",
    "        tf = counter[token]\n",
    "        try:\n",
    "          idf_temp=idf[token]\n",
    "        except:\n",
    "          pass\n",
    "        try:\n",
    "            ind = columns_name.index(token)\n",
    "            Q[ind] = tf*idf_temp\n",
    "        except:\n",
    "            pass\n",
    "    return Q\n",
    "\n",
    "# function to calculate tf_idf_score\n",
    "def tf_idf_score(k, query):\n",
    "    query_weights = {}\n",
    "    for key in tf_idf:\n",
    "        if key[1] in query:\n",
    "            try:\n",
    "                query_weights[key[0]] += tf_idf[key]\n",
    "            except:\n",
    "                query_weights[key[0]] = tf_idf[key]\n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "  \n",
    "    l = []\n",
    "    for i in query_weights[:k]:\n",
    "        l.append(i)\n",
    "    return l\n",
    "\n",
    "# function to calculte Cosine Similarity \n",
    "def cosine_similarity(k, query):\n",
    "    d_cosines = []\n",
    "    query_vector = gen_vector(query)\n",
    "    for d in D:\n",
    "        d_cosines.append(cosine_dot(query_vector, d))\n",
    "    out = np.array(d_cosines).argsort()[-k:][::-1]\n",
    "  \n",
    "    final_display_disease={}\n",
    "    for lt in set(out):\n",
    "      final_display_disease[lt] = float(d_cosines[lt])\n",
    "    return final_display_disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pyd1WbBq5Ngh"
   },
   "outputs": [],
   "source": [
    "# returns the list of synonyms of the input word from thesaurus.com (https://www.thesaurus.com/) and wordnet (https://www.nltk.org/howto/wordnet.html)\n",
    "def synonyms(term):\n",
    "    synonyms = []\n",
    "    response = requests.get('https://www.thesaurus.com/browse/{}'.format(term))\n",
    "    soup = BeautifulSoup(response.content,  \"html.parser\")\n",
    "    try:\n",
    "        container=soup.find('section', {'class': 'MainContentContainer'}) \n",
    "        row=container.find('div',{'class':'css-191l5o0-ClassicContentCard'})\n",
    "        row = row.find_all('li')\n",
    "        for x in row:\n",
    "            synonyms.append(x.get_text())\n",
    "    except:\n",
    "        None\n",
    "    for syn in wordnet.synsets(term):\n",
    "        synonyms+=syn.lemma_names()\n",
    "    return set(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXPUlgHi63Zu"
   },
   "outputs": [],
   "source": [
    "# instantiate objects of libraries\n",
    "splitter = RegexpTokenizer(r'\\w+')\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DGtPYRLhey0y"
   },
   "source": [
    "**Disease Symptom dataset** was created in a separate python program.\n",
    "\n",
    "**Dataset scrapping** was done using **NHP website** and **wikipedia data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rZTXyRhNgN_O"
   },
   "source": [
    "Disease Combination dataset contains the combinations for each of the disease present in dataset as practically it is often observed that it is not necessary for a person to have a disease when all the symptoms are faced by the patient or the user.\n",
    "\n",
    "*To tackle this problem, combinations are made with the symptoms for each disease.*\n",
    "\n",
    " **This increases the size of the data exponentially and helps the model to predict the disease with much better accuracy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h1LSI08aiDTn"
   },
   "source": [
    "*df_comb -> Dataframe consisting of dataset generated by combining symptoms for each disease.*\n",
    "\n",
    "*df_norm -> Dataframe consisting of dataset which contains a single row for each diseases with all the symptoms for that corresponding disease.*\n",
    "\n",
    "**Dataset contains 261 diseases and their symptoms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpK73qQx5NmJ"
   },
   "outputs": [],
   "source": [
    "# Load Dataset scraped from NHP (https://www.nhp.gov.in/disease-a-z) & Wikipedia\n",
    "# Scrapping and creation of dataset csv is done in a separate program\n",
    "df_comb = pd.read_csv(\"/content/drive/My Drive/Python Project data/IR_Project/dis_sym_dataset_comb.csv\") # Disease combination\n",
    "df_norm = pd.read_csv(\"/content/drive/My Drive/Python Project data/IR_Project/dis_sym_dataset_norm.csv\") # Individual Disease\n",
    "Y = df_norm.iloc[:, 0:1]\n",
    "X = df_norm.iloc[:, 1:]\n",
    "# List of symptoms\n",
    "dataset_symptoms = list(X.columns)\n",
    "diseases = list(set(Y['label_dis']))\n",
    "diseases.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "yd8K2QeH5NsL",
    "outputId": "62571279-7471-4698-f233-6e0a97c34979"
   },
   "outputs": [],
   "source": [
    "# Taking symptoms from user as input\n",
    "# Preprocessing the input symtoms \n",
    "user_symptoms = str(input(\"\\nPlease enter symptoms separated by comma(,):\\n\")).lower().split(',')\n",
    "processed_user_symptoms=[]\n",
    "for sym in user_symptoms:\n",
    "    sym=sym.strip()\n",
    "    sym=sym.replace('-',' ')\n",
    "    sym=sym.replace(\"'\",'')\n",
    "    sym = ' '.join([lemmatizer.lemmatize(word) for word in splitter.tokenize(sym)])\n",
    "    processed_user_symptoms.append(sym)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "THRf6-Wa5Nxu",
    "outputId": "122d0bcb-569f-4610-f713-76382d03fb3c"
   },
   "outputs": [],
   "source": [
    "# Taking each user symptom and finding all its synonyms and appending it to the pre-processed symptom string\n",
    "user_symptoms = []\n",
    "for user_sym in processed_user_symptoms:\n",
    "    user_sym = user_sym.split()\n",
    "    str_sym = set()\n",
    "    for comb in range(1, len(user_sym)+1):\n",
    "        for subset in combinations(user_sym, comb):\n",
    "            subset=' '.join(subset)\n",
    "            subset = synonyms(subset) \n",
    "            str_sym.update(subset)\n",
    "    str_sym.add(' '.join(user_sym))\n",
    "    user_symptoms.append(' '.join(str_sym).replace('_',' '))\n",
    "# query expansion performed by joining synonyms found for each symptoms initially entered\n",
    "print(\"After query expansion done by using the symptoms entered\")\n",
    "print(user_symptoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7sPyVlJIjdv2"
   },
   "source": [
    "The below procedure is performed in order to show the symptom synonmys found for the symptoms entered by the user.\n",
    "\n",
    "The symptom synonyms and user symptoms are matched with the symptoms present in dataset. Only the symptoms which matches the symptoms present in dataset are shown back to the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tFUYSnLU5Nu-"
   },
   "outputs": [],
   "source": [
    "# Loop over all the symptoms in dataset and check its similarity score to the synonym string of the user-input \n",
    "# symptoms. If similarity>0.5, add the symptom to the final list\n",
    "found_symptoms = set()\n",
    "for idx, data_sym in enumerate(dataset_symptoms):\n",
    "    data_sym_split=data_sym.split()\n",
    "    for user_sym in user_symptoms:\n",
    "        count=0\n",
    "        for symp in data_sym_split:\n",
    "            if symp in user_sym.split():\n",
    "                count+=1\n",
    "        if count/len(data_sym_split)>0.5:\n",
    "            found_symptoms.add(data_sym)\n",
    "found_symptoms = list(found_symptoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "jBV1qPFv5NpY",
    "outputId": "bc96a52e-15fc-496a-c333-5d195a4fe430"
   },
   "outputs": [],
   "source": [
    "# Print all found symptoms\n",
    "print(\"Top matching symptoms from your search!\")\n",
    "for idx, symp in enumerate(found_symptoms):\n",
    "    print(idx,\":\",symp)\n",
    "\n",
    "# Show the related symptoms found in the dataset and ask user to select among them\n",
    "select_list = input(\"\\nPlease select the relevant symptoms. Enter indices (separated-space):\\n\").split()\n",
    "\n",
    "# Find other relevant symptoms from the dataset based on user symptoms based on the highest co-occurance with the\n",
    "# ones that is input by the user\n",
    "dis_list = set()\n",
    "final_symp = [] \n",
    "counter_list = []\n",
    "for idx in select_list:\n",
    "    symp=found_symptoms[int(idx)]\n",
    "    final_symp.append(symp)\n",
    "    dis_list.update(set(df_norm[df_norm[symp]==1]['label_dis']))\n",
    "   \n",
    "for dis in dis_list:\n",
    "    row = df_norm.loc[df_norm['label_dis'] == dis].values.tolist()\n",
    "    row[0].pop(0)\n",
    "    for idx,val in enumerate(row[0]):\n",
    "        if val!=0 and dataset_symptoms[idx] not in final_symp:\n",
    "            counter_list.append(dataset_symptoms[idx])\n",
    "\n",
    "# Symptoms that co-occur with the ones selected by user              \n",
    "dict_symp = dict(Counter(counter_list))\n",
    "dict_symp_tup = sorted(dict_symp.items(), key=operator.itemgetter(1),reverse=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "colab_type": "code",
    "id": "SgvzIn7Q5NjV",
    "outputId": "d0da9e18-ff49-4cd1-f486-983f06140917"
   },
   "outputs": [],
   "source": [
    "# Iteratively, suggest top co-occuring symptoms to the user and ask to select the ones applicable \n",
    "found_symptoms=[]\n",
    "count=0\n",
    "for tup in dict_symp_tup:\n",
    "    count+=1\n",
    "    found_symptoms.append(tup[0])\n",
    "    if count%5==0 or count==len(dict_symp_tup):\n",
    "        print(\"\\nCommon co-occuring symptoms:\")\n",
    "        for idx,ele in enumerate(found_symptoms):\n",
    "            print(idx,\":\",ele)\n",
    "        select_list = input(\"Do you have have of these symptoms? If Yes, enter the indices (space-separated), 'no' to stop, '-1' to skip:\\n\").lower().split();\n",
    "        if select_list[0]=='no':\n",
    "            break\n",
    "        if select_list[0]=='-1':\n",
    "            found_symptoms = [] \n",
    "            continue\n",
    "        for idx in select_list:\n",
    "            final_symp.append(found_symptoms[int(idx)])\n",
    "        found_symptoms = []    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nI5taHc8pfY3"
   },
   "source": [
    "Final Symptom list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "YYPReN9D5Nd_",
    "outputId": "af695963-a0e3-4e76-c2a9-4a7c2ca75442"
   },
   "outputs": [],
   "source": [
    "#Calculating TF-IDF and Cosine Similarity using matched symptoms\n",
    "k = 10\n",
    "\n",
    "print(\"Final list of Symptoms used for prediction are : \")\n",
    "for val in final_symp:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8_A-6Dl5qHlv"
   },
   "source": [
    "# **Showing the list of top k diseases to the user with their prediction probabilities.**\n",
    "\n",
    "# **For getting information about the suggested treatments, user can enter the corresponding index to know more details.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "tWUsVkF3t6jk",
    "outputId": "7baabb86-a3b9-4e7b-e7ce-fc4818ea9de3"
   },
   "outputs": [],
   "source": [
    "topk1=tf_idf_score(k,final_symp)\n",
    "topk2=cosine_similarity(k,final_symp)\n",
    "# Show top 10 highly probable disease to the user.\n",
    "print(f\"\\nTop {k} diseases predicted based on TF_IDF Matching :\\n\")\n",
    "i = 0\n",
    "topk1_index_mapping = {}\n",
    "for key, score in topk1:\n",
    "  print(f\"{i}. Disease : {key} \\t Score : {round(score, 2)}\")\n",
    "  topk1_index_mapping[i] = key\n",
    "  i += 1\n",
    "\n",
    "select = input(\"\\nMore details about the disease? Enter index of disease or '-1' to discontinue:\\n\")\n",
    "if select!='-1':\n",
    "    dis=topk1_index_mapping[int(select)]\n",
    "    print()\n",
    "    print(diseaseDetail(dis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "colab_type": "code",
    "id": "AHiGmHhqdnMs",
    "outputId": "70f583d0-83e5-4424-8849-c97fbb189c41"
   },
   "outputs": [],
   "source": [
    "# display top k diseases predicted with cosine probablity\n",
    "print(f\"Top {k} disease based on Cosine Similarity Matching :\\n \")\n",
    "topk2_sorted = dict(sorted(topk2.items(), key=lambda kv: kv[1], reverse=True))\n",
    "j = 0\n",
    "topk2_index_mapping = {}\n",
    "for key in topk2_sorted:\n",
    "  print(f\"{j}. Disease : {diseases[key]} \\t Score : {round(topk2_sorted[key], 2)}\")\n",
    "  topk2_index_mapping[j] = diseases[key]\n",
    "  j += 1\n",
    "\n",
    "    \n",
    "select = input(\"\\nMore details about the disease? Enter index of disease or '-1' to discontinue and close the system:\\n\")\n",
    "if select!='-1':\n",
    "    dis=topk2_index_mapping[int(select)]\n",
    "    print()\n",
    "    print(diseaseDetail(dis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ohuPaboMyKXa"
   },
   "source": [
    "# New Section\n",
    "**NEURAL_NETWORK AND GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "p9pStYfV_FGM",
    "outputId": "b509778c-015e-4f20-8e65-be400cc81d20"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install neural_structured_learning\n",
    "#importing all libraries\n",
    "import neural_structured_learning as nsl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras import initializers\n",
    "from keras.optimizers import SGD\n",
    "#import neural_structured_learning as nsl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4TQqjkbmymrb"
   },
   "outputs": [],
   "source": [
    "#reading Dataset and making dataframe\n",
    "datat=pd.read_csv('/content/drive/My Drive/Python Project data/IR_Project/dis_sym_dataset_comb.csv')\n",
    "df_new=pd.DataFrame(datat)\n",
    "df_new=df_new.sample(frac=1)\n",
    "#print(df_new)\n",
    "Y=df_new['label_dis']\n",
    "X=df_new.drop(columns='label_dis',axis=1)\n",
    "total_symptoms_len=len(X.columns)\n",
    "total_disease_len=len(set(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AJGNzECTynHS"
   },
   "outputs": [],
   "source": [
    "#Label Encoding Class to numeric type \n",
    "#Converting class to categorical type for categorical cross entropy\n",
    "lb=LabelEncoder()\n",
    "Y=lb.fit_transform(Y)\n",
    "Ycat=to_categorical(Y)\n",
    "X=np.array(X)\n",
    "Y=np.array(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iB0KNQoRynNL"
   },
   "outputs": [],
   "source": [
    "#importing tensorflow and keras frameworks\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#base Model Neurel Net\n",
    "def base_model():\n",
    "  inputs=keras.Input(shape=(total_symptoms_len,),dtype=tf.float32,name=IMAGE_INPUT_NAME)#defining input shape and dtype \n",
    "  x=inputs\n",
    "  x=keras.layers.Dense(1000,activation='relu',use_bias=True,kernel_initializer=initializers.he_normal(seed=None))(x)#Dense layer relu\n",
    "\n",
    "  x=keras.layers.Dense(1000,activation='relu',use_bias=True,kernel_initializer=initializers.he_normal(seed=None))(x)#Dense layer relu\n",
    "\n",
    "  outputs=keras.layers.Dense(total_disease_len,activation='softmax')(x)#output Dense layer with class size\n",
    "\n",
    "  model=keras.Model(inputs=inputs,outputs=outputs,name='NN_sequential_model')#creating model\n",
    "\n",
    "  #model.add(Dense(1500,activation='relu',kernel_initializer='he_uniform'))\n",
    "  # model.add(Dense(500,activation='relu',use_bias=True,kernel_initializer=initializers.he_normal(seed=None)))\n",
    "  # model.add(Dense(183,activation='softmax'))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7xYNar3nynKf"
   },
   "outputs": [],
   "source": [
    " def convert_to_dictionaries(image, label):\n",
    "  return {IMAGE_INPUT_NAME: image, LABEL_INPUT_NAME: label}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iHSec4ggynSr"
   },
   "outputs": [],
   "source": [
    "IMAGE_INPUT_NAME = 'image'\n",
    "LABEL_INPUT_NAME = 'label'\n",
    "#making adversarial Configurations for training\n",
    "adv_config = nsl.configs.make_adv_reg_config(\n",
    "    multiplier=0.2,\n",
    "    adv_step_size=0.0001\n",
    ")\n",
    "base_adv_model =base_model()#calling base model\n",
    "#building adversiaral graphs for embedding and combining with base modrl\n",
    "adv_model = nsl.keras.AdversarialRegularization(\n",
    "    base_adv_model,\n",
    "    label_keys=[LABEL_INPUT_NAME],\n",
    "    adv_config=adv_config\n",
    ")\n",
    "train_set_for_adv_model = convert_to_dictionaries(X,Ycat)#converting it to dictionary for training adversiarial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AsVGBV7YynVG",
    "outputId": "3094a75e-a221-47bd-f933-5f9d82052617"
   },
   "outputs": [],
   "source": [
    "base_mod=base_model()\n",
    "base_mod.summary()\n",
    "es=tf.keras.callbacks.EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=10)#early stopping\n",
    "mc = tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)#saving best model\n",
    "print(\"Normal Feed Forward Neural Network\")\n",
    "base_mod.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "history=base_mod.fit(X,Ycat,validation_split=0.2,epochs=20,verbose=1,callbacks=[es,mc])#training Neural Network\n",
    "base_mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OPNvVLv1ymql",
    "outputId": "4a840e11-68a2-4e42-fbf1-56979a09201c"
   },
   "outputs": [],
   "source": [
    "adv_model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "                   metrics=['acc'])\n",
    "es=tf.keras.callbacks.EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=10)\n",
    "mc = tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_categorical_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "print(\"applied adversarial regularization on base neural network\")\n",
    "#adv_model.compile(optimizer='adam', loss='categorical_cross_entropy', metrics=['accuracy'])\n",
    "adv_model.fit(train_set_for_adv_model,validation_split=0.2 ,epochs=15,callbacks=[es,mc])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TF_IDF_NN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
